{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b98a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 09:40:46.266322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,cross_val_score, GridSearchCV\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from optbinning import BinningProcess, OptimalBinning # Para cáclulos WOE e IV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6486cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>momento</th>\n",
       "      <th>1-0005</th>\n",
       "      <th>5-0005</th>\n",
       "      <th>1-_006</th>\n",
       "      <th>5-_006</th>\n",
       "      <th>1-_007</th>\n",
       "      <th>5-_007</th>\n",
       "      <th>1-_008</th>\n",
       "      <th>5-_008</th>\n",
       "      <th>1-_010</th>\n",
       "      <th>...</th>\n",
       "      <th>1-_098</th>\n",
       "      <th>5-_098</th>\n",
       "      <th>1-_099</th>\n",
       "      <th>5-_099</th>\n",
       "      <th>1-0112</th>\n",
       "      <th>5-0112</th>\n",
       "      <th>1-0116</th>\n",
       "      <th>5-0116</th>\n",
       "      <th>1-0109</th>\n",
       "      <th>5-0109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-588.88</td>\n",
       "      <td>-580.21</td>\n",
       "      <td>-590.50</td>\n",
       "      <td>-582.25</td>\n",
       "      <td>-593.51</td>\n",
       "      <td>-585.46</td>\n",
       "      <td>-595.51</td>\n",
       "      <td>-587.47</td>\n",
       "      <td>-575.36</td>\n",
       "      <td>...</td>\n",
       "      <td>-588.17</td>\n",
       "      <td>-581.42</td>\n",
       "      <td>-594.26</td>\n",
       "      <td>-586.26</td>\n",
       "      <td>-589.46</td>\n",
       "      <td>-581.75</td>\n",
       "      <td>-592.92</td>\n",
       "      <td>-586.26</td>\n",
       "      <td>-593.93</td>\n",
       "      <td>-586.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-588.38</td>\n",
       "      <td>-579.71</td>\n",
       "      <td>-590.80</td>\n",
       "      <td>-582.46</td>\n",
       "      <td>-593.80</td>\n",
       "      <td>-585.67</td>\n",
       "      <td>-595.55</td>\n",
       "      <td>-587.59</td>\n",
       "      <td>-574.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-588.34</td>\n",
       "      <td>-581.63</td>\n",
       "      <td>-594.51</td>\n",
       "      <td>-586.26</td>\n",
       "      <td>-589.29</td>\n",
       "      <td>-581.58</td>\n",
       "      <td>-592.80</td>\n",
       "      <td>-586.26</td>\n",
       "      <td>-594.13</td>\n",
       "      <td>-586.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-584.41</td>\n",
       "      <td>-575.74</td>\n",
       "      <td>-585.83</td>\n",
       "      <td>-577.66</td>\n",
       "      <td>-591.38</td>\n",
       "      <td>-583.21</td>\n",
       "      <td>-589.84</td>\n",
       "      <td>-578.04</td>\n",
       "      <td>-563.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-588.17</td>\n",
       "      <td>-581.33</td>\n",
       "      <td>-592.88</td>\n",
       "      <td>-584.34</td>\n",
       "      <td>-588.50</td>\n",
       "      <td>-580.62</td>\n",
       "      <td>-590.21</td>\n",
       "      <td>-583.75</td>\n",
       "      <td>-593.68</td>\n",
       "      <td>-585.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-573.73</td>\n",
       "      <td>-565.27</td>\n",
       "      <td>-570.31</td>\n",
       "      <td>-562.81</td>\n",
       "      <td>-578.91</td>\n",
       "      <td>-571.24</td>\n",
       "      <td>-578.61</td>\n",
       "      <td>-565.90</td>\n",
       "      <td>-555.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-584.79</td>\n",
       "      <td>-576.87</td>\n",
       "      <td>-580.62</td>\n",
       "      <td>-568.15</td>\n",
       "      <td>-586.96</td>\n",
       "      <td>-578.75</td>\n",
       "      <td>-578.66</td>\n",
       "      <td>-571.19</td>\n",
       "      <td>-590.88</td>\n",
       "      <td>-582.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-567.27</td>\n",
       "      <td>-558.55</td>\n",
       "      <td>-562.26</td>\n",
       "      <td>-554.55</td>\n",
       "      <td>-570.69</td>\n",
       "      <td>-562.93</td>\n",
       "      <td>-567.18</td>\n",
       "      <td>-557.51</td>\n",
       "      <td>-545.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-579.11</td>\n",
       "      <td>-570.99</td>\n",
       "      <td>-572.11</td>\n",
       "      <td>-560.43</td>\n",
       "      <td>-583.45</td>\n",
       "      <td>-574.91</td>\n",
       "      <td>-571.06</td>\n",
       "      <td>-563.56</td>\n",
       "      <td>-587.38</td>\n",
       "      <td>-578.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>637</td>\n",
       "      <td>-237.18</td>\n",
       "      <td>-253.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>638</td>\n",
       "      <td>-237.10</td>\n",
       "      <td>-253.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>639</td>\n",
       "      <td>-237.14</td>\n",
       "      <td>-253.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>640</td>\n",
       "      <td>-237.10</td>\n",
       "      <td>-253.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>641</td>\n",
       "      <td>-237.10</td>\n",
       "      <td>-253.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>641 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     momento  1-0005  5-0005  1-_006  5-_006  1-_007  5-_007  1-_008  5-_008  \\\n",
       "0          1 -588.88 -580.21 -590.50 -582.25 -593.51 -585.46 -595.51 -587.47   \n",
       "1          2 -588.38 -579.71 -590.80 -582.46 -593.80 -585.67 -595.55 -587.59   \n",
       "2          3 -584.41 -575.74 -585.83 -577.66 -591.38 -583.21 -589.84 -578.04   \n",
       "3          4 -573.73 -565.27 -570.31 -562.81 -578.91 -571.24 -578.61 -565.90   \n",
       "4          5 -567.27 -558.55 -562.26 -554.55 -570.69 -562.93 -567.18 -557.51   \n",
       "..       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "636      637 -237.18 -253.66     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "637      638 -237.10 -253.70     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "638      639 -237.14 -253.74     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "639      640 -237.10 -253.70     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "640      641 -237.10 -253.74     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "     1-_010  ...  1-_098  5-_098  1-_099  5-_099  1-0112  5-0112  1-0116  \\\n",
       "0   -575.36  ... -588.17 -581.42 -594.26 -586.26 -589.46 -581.75 -592.92   \n",
       "1   -574.11  ... -588.34 -581.63 -594.51 -586.26 -589.29 -581.58 -592.80   \n",
       "2   -563.34  ... -588.17 -581.33 -592.88 -584.34 -588.50 -580.62 -590.21   \n",
       "3   -555.00  ... -584.79 -576.87 -580.62 -568.15 -586.96 -578.75 -578.66   \n",
       "4   -545.99  ... -579.11 -570.99 -572.11 -560.43 -583.45 -574.91 -571.06   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "636     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "637     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "638     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "639     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "640     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "     5-0116  1-0109  5-0109  \n",
       "0   -586.26 -593.93 -586.30  \n",
       "1   -586.26 -594.13 -586.51  \n",
       "2   -583.75 -593.68 -585.84  \n",
       "3   -571.19 -590.88 -582.63  \n",
       "4   -563.56 -587.38 -578.71  \n",
       "..      ...     ...     ...  \n",
       "636     NaN     NaN     NaN  \n",
       "637     NaN     NaN     NaN  \n",
       "638     NaN     NaN     NaN  \n",
       "639     NaN     NaN     NaN  \n",
       "640     NaN     NaN     NaN  \n",
       "\n",
       "[641 rows x 77 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('vacuum_sensor_data.csv', sep = ';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column to have consistent naming \n",
    "import re\n",
    "\n",
    "def clean_column(col):\n",
    "    if col == 'momento':\n",
    "        return col\n",
    "    match = re.match(r\"(\\d)-_?(\\d+)\", col)\n",
    "    if match:\n",
    "        sensor, comp = match.groups()\n",
    "        return f\"{sensor}-{int(comp):04d}\"\n",
    "    return col  # fallback in case format is already correct\n",
    "\n",
    "# Apply renaming\n",
    "data.columns = [clean_column(col) for col in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daeb3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows(seconds) where data from any sensor is missing\n",
    "data=data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b335815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 77)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ddda49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0005 data shape: (533, 2)\n",
      "Component 0006 data shape: (533, 2)\n",
      "Component 0007 data shape: (533, 2)\n",
      "Component 0008 data shape: (533, 2)\n",
      "Component 0010 data shape: (533, 2)\n",
      "Component 0011 data shape: (533, 2)\n",
      "Component 0016 data shape: (533, 2)\n",
      "Component 0018 data shape: (533, 2)\n",
      "Component 0019 data shape: (533, 2)\n",
      "Component 0021 data shape: (533, 2)\n",
      "Component 0035 data shape: (533, 2)\n",
      "Component 0038 data shape: (533, 2)\n",
      "Component 0039 data shape: (533, 2)\n",
      "Component 0041 data shape: (533, 2)\n",
      "Component 0043 data shape: (533, 2)\n",
      "Component 0045 data shape: (533, 2)\n",
      "Component 0047 data shape: (533, 2)\n",
      "Component 0050 data shape: (533, 2)\n",
      "Component 0064 data shape: (533, 2)\n",
      "Component 0065 data shape: (533, 2)\n",
      "Component 0066 data shape: (533, 2)\n",
      "Component 0067 data shape: (533, 2)\n",
      "Component 0071 data shape: (533, 2)\n",
      "Component 0076 data shape: (533, 2)\n",
      "Component 0078 data shape: (533, 2)\n",
      "Component 0079 data shape: (533, 2)\n",
      "Component 0083 data shape: (533, 2)\n",
      "Component 0084 data shape: (533, 2)\n",
      "Component 0089 data shape: (533, 2)\n",
      "Component 0092 data shape: (533, 2)\n",
      "Component 0095 data shape: (533, 2)\n",
      "Component 0096 data shape: (533, 2)\n",
      "Component 0097 data shape: (533, 2)\n",
      "Component 0098 data shape: (533, 2)\n",
      "Component 0099 data shape: (533, 2)\n",
      "Component 0109 data shape: (533, 2)\n",
      "Component 0112 data shape: (533, 2)\n",
      "Component 0116 data shape: (533, 2)\n",
      "Shape of X_train_sequences: (34, 201, 2)\n",
      "Shape of X_validation_sequences: (4, 201, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script prepares sensor data for LSTM Autoencoder training and validation.\n",
    "It extracts time series for each component (sensor 1 and sensor 5 readings),\n",
    "splits them into training and validation sets, normalizes the data, and reshapes\n",
    "it into sequences suitable for LSTM input.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Apply the time window filtering first\n",
    "# The user specified 'momento 50-250', which corresponds to rows 49 to 249 (inclusive) in 0-indexed pandas.\n",
    "# So, data.iloc[49:250] is correct for 201 timesteps.\n",
    "data_full = data.iloc[49:250].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Define defective and normal control columns\n",
    "defective_columns = [\"1-0116\", \"5-0116\", \"1-0109\", \"5-0109\"]\n",
    "normal_control_columns = [\"1-0008\", \"5-0008\", \"1-0064\", \"5-0064\"]\n",
    "\n",
    "# Separate momento column (if needed for later, but not for feature extraction)\n",
    "momento_data = data[\"momento\"]\n",
    "sensor_data_raw = data.drop(columns=[\"momento\"])\n",
    "\n",
    "# Create a dictionary to store time series for each component\n",
    "component_time_series = {}\n",
    "# Extract unique component IDs from column names (e.g., '0005', '0006', etc.)\n",
    "all_component_ids = sorted(list(set([col.split(\"-\")[1] for col in sensor_data_raw.columns])))\n",
    "\n",
    "# Iterate through each component ID and extract its sensor 1 and sensor 5 data\n",
    "for comp_id in all_component_ids:\n",
    "    s1_col = f\"1-{comp_id}\"\n",
    "    s5_col = f\"5-{comp_id}\"\n",
    "    \n",
    "    # Check if both sensor columns exist for this component in the raw data\n",
    "    if s1_col in sensor_data_raw.columns and s5_col in sensor_data_raw.columns:\n",
    "        component_data = sensor_data_raw[[s1_col, s5_col]].values\n",
    "        component_time_series[comp_id] = component_data\n",
    "        print(f\"Component {comp_id} data shape: {component_data.shape}\") # Debugging line\n",
    "\n",
    "# Split components into training and validation sets based on their IDs\n",
    "defective_component_ids = sorted(list(set([col.split(\"-\")[1] for col in defective_columns])))\n",
    "normal_control_component_ids = sorted(list(set([col.split(\"-\")[1] for col in normal_control_columns])))\n",
    "\n",
    "# Training components are all components that are not in defective_component_ids or normal_control_component_ids\n",
    "train_component_ids = [comp_id for comp_id in all_component_ids \n",
    "                       if comp_id not in defective_component_ids and comp_id not in normal_control_component_ids]\n",
    "\n",
    "# Validation components are defective_component_ids + normal_control_component_ids\n",
    "validation_component_ids = defective_component_ids + normal_control_component_ids\n",
    "\n",
    "# Prepare training data\n",
    "X_train_list = []\n",
    "for comp_id in train_component_ids:\n",
    "    if comp_id in component_time_series:\n",
    "        X_train_list.append(component_time_series[comp_id])\n",
    "\n",
    "# Concatenate all training time series into a single array for scaling\n",
    "# This assumes all components have the same number of timesteps (rows).\n",
    "# The `iloc[49:250]` slicing should ensure this, resulting in 201 timesteps.\n",
    "X_train_flat = np.vstack(X_train_list)\n",
    "\n",
    "# Normalize the training data (fit scaler on all normal component sensor readings)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "\n",
    "# Reshape back into sequences for each component\n",
    "# The number of timesteps per component should be consistent (201).\n",
    "TIMESTEPS_PER_COMPONENT = 201 \n",
    "\n",
    "# Reconstruct X_train_sequences from the scaled flat array\n",
    "X_train_sequences = []\n",
    "current_idx = 0\n",
    "for comp_id in train_component_ids:\n",
    "    if comp_id in component_time_series:\n",
    "        # Ensure the slice has the expected number of timesteps\n",
    "        expected_end_idx = current_idx + TIMESTEPS_PER_COMPONENT\n",
    "        if expected_end_idx <= len(X_train_scaled_flat):\n",
    "            X_train_sequences.append(X_train_scaled_flat[current_idx : expected_end_idx])\n",
    "            current_idx += TIMESTEPS_PER_COMPONENT\n",
    "        else:\n",
    "            print(f\"Warning: Component {comp_id} has fewer than {TIMESTEPS_PER_COMPONENT} timesteps after slicing.\")\n",
    "            # Handle this case, e.g., by padding or skipping, or investigate why length is inconsistent.\n",
    "            # For now, let's assume it's an error and will be caught if np.array fails.\n",
    "\n",
    "X_train_sequences = np.array(X_train_sequences) # This is where the ValueError occurred previously\n",
    "\n",
    "# Prepare validation data\n",
    "X_validation_list = []\n",
    "for comp_id in validation_component_ids:\n",
    "    if comp_id in component_time_series:\n",
    "        X_validation_list.append(component_time_series[comp_id])\n",
    "\n",
    "# Scale validation data using the *same* scaler\n",
    "X_validation_flat = np.vstack(X_validation_list)\n",
    "X_validation_scaled_flat = scaler.transform(X_validation_flat)\n",
    "\n",
    "# Reconstruct X_validation_sequences from the scaled flat array\n",
    "X_validation_sequences = []\n",
    "current_idx = 0\n",
    "for comp_id in validation_component_ids:\n",
    "    if comp_id in component_time_series:\n",
    "        expected_end_idx = current_idx + TIMESTEPS_PER_COMPONENT\n",
    "        if expected_end_idx <= len(X_validation_scaled_flat):\n",
    "            X_validation_sequences.append(X_validation_scaled_flat[current_idx : expected_end_idx])\n",
    "            current_idx += TIMESTEPS_PER_COMPONENT\n",
    "        else:\n",
    "            print(f\"Warning: Validation component {comp_id} has fewer than {TIMESTEPS_PER_COMPONENT} timesteps after slicing.\")\n",
    "\n",
    "X_validation_sequences = np.array(X_validation_sequences)\n",
    "\n",
    "print(\"Shape of X_train_sequences:\", X_train_sequences.shape)\n",
    "print(\"Shape of X_validation_sequences:\", X_validation_sequences.shape)\n",
    "\n",
    "# Save the processed data and component IDs\n",
    "np.save(\"X_train_sequences.npy\", X_train_sequences)\n",
    "np.save(\"X_validation_sequences.npy\", X_validation_sequences)\n",
    "np.save(\"validation_component_ids.npy\", np.array(validation_component_ids))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31be1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.2258 - val_loss: 0.1684\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 0.2213 - val_loss: 0.1644\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 0.2167 - val_loss: 0.1604\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.2121 - val_loss: 0.1562\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.2074 - val_loss: 0.1521\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 0.2026 - val_loss: 0.1479\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 0.1977 - val_loss: 0.1437\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.1927 - val_loss: 0.1392\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.1876 - val_loss: 0.1347\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 0.1823 - val_loss: 0.1301\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 0.1769 - val_loss: 0.1254\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - loss: 0.1714 - val_loss: 0.1208\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 0.1657 - val_loss: 0.1160\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.1599 - val_loss: 0.1111\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.1540 - val_loss: 0.1061\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.1480 - val_loss: 0.1010\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.1416 - val_loss: 0.0956\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.1351 - val_loss: 0.0902\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 0.1284 - val_loss: 0.0847\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 0.1215 - val_loss: 0.0789\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.1143 - val_loss: 0.0730\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 0.1067 - val_loss: 0.0671\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 0.0990 - val_loss: 0.0615\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - loss: 0.0914 - val_loss: 0.0563\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.0843 - val_loss: 0.0518\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 0.0781 - val_loss: 0.0487\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 0.0738 - val_loss: 0.0477\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0728 - val_loss: 0.0497\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - loss: 0.0775 - val_loss: 0.0521\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - loss: 0.0827 - val_loss: 0.0519\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.0822 - val_loss: 0.0500\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0781 - val_loss: 0.0477\n",
      "Model training complete and saved as lstm_autoencoder_model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the prepared data\n",
    "X_train_sequences = np.load(\"X_train_sequences.npy\")\n",
    "\n",
    "# Define the LSTM Autoencoder model\n",
    "def create_lstm_autoencoder(timesteps, n_features, latent_dim=32):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(timesteps, n_features))\n",
    "    encoded = LSTM(latent_dim, activation='relu')(inputs)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = LSTM(n_features, activation='relu', return_sequences=True)(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "timesteps = X_train_sequences.shape[1]\n",
    "n_features = X_train_sequences.shape[2]\n",
    "\n",
    "model = create_lstm_autoencoder(timesteps, n_features)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "# For an autoencoder, the input and output are the same for training\n",
    "history = model.fit(X_train_sequences, X_train_sequences, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_split=0.1, \n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Save the trained model in the native Keras format\n",
    "model.save(\"lstm_autoencoder_model.keras\")\n",
    "\n",
    "print(\"Model training complete and saved as lstm_autoencoder_model.keras\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b56886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Anomaly Threshold: 0.07526985399337532\n",
      "Number of anomalous components: 1\n",
      "Anomalous Component IDs: ['0008']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"lstm_autoencoder_model.keras\")\n",
    "\n",
    "# Load the prepared data\n",
    "X_train_sequences = np.load(\"X_train_sequences.npy\")\n",
    "X_validation_sequences = np.load(\"X_validation_sequences.npy\")\n",
    "validation_component_ids = np.load(\"validation_component_ids.npy\")\n",
    "\n",
    "# Predict reconstructions for training data\n",
    "X_train_pred = model.predict(X_train_sequences)\n",
    "\n",
    "# Calculate reconstruction errors for training data (per sequence)\n",
    "train_reconstruction_errors = np.array([mean_squared_error(X_train_sequences[i].flatten(), X_train_pred[i].flatten())\n",
    "                                        for i in range(len(X_train_sequences))])\n",
    "\n",
    "# Predict reconstructions for validation data\n",
    "X_validation_pred = model.predict(X_validation_sequences)\n",
    "\n",
    "# Calculate reconstruction errors for validation data (per component)\n",
    "# Each entry in X_validation_sequences corresponds to a component.\n",
    "component_reconstruction_errors = []\n",
    "for i in range(len(X_validation_sequences)):\n",
    "    error = mean_squared_error(X_validation_sequences[i].flatten(), X_validation_pred[i].flatten())\n",
    "    component_reconstruction_errors.append(error)\n",
    "\n",
    "component_reconstruction_errors = np.array(component_reconstruction_errors)\n",
    "\n",
    "# Determine an anomaly threshold based on training data reconstruction errors\n",
    "anomaly_threshold = np.percentile(train_reconstruction_errors, 75)\n",
    "print(f\"Anomaly Threshold: {anomaly_threshold}\")\n",
    "\n",
    "# Identify anomalous components\n",
    "anomalous_components_mask = component_reconstruction_errors > anomaly_threshold\n",
    "anomalous_component_ids = validation_component_ids[anomalous_components_mask]\n",
    "\n",
    "print(f\"Number of anomalous components: {len(anomalous_component_ids)}\")\n",
    "print(f\"Anomalous Component IDs: {anomalous_component_ids}\")\n",
    "\n",
    "# Visualize reconstruction errors for components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(validation_component_ids, component_reconstruction_errors, color='skyblue')\n",
    "plt.axhline(anomaly_threshold, color='r', linestyle='--', label='Anomaly Threshold')\n",
    "plt.xlabel('Component ID')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error per Validation Component')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('component_reconstruction_errors.png')\n",
    "plt.close()\n",
    "\n",
    "# Optional: Visualize distribution of training errors (same as before)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_reconstruction_errors, bins=50, density=True, alpha=0.6, label='Training Reconstruction Errors')\n",
    "plt.axvline(anomaly_threshold, color='r', linestyle='--', label='Anomaly Threshold')\n",
    "plt.title('Training Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.savefig('training_reconstruction_error_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8f995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bee37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaab687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0521b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
